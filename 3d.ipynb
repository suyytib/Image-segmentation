{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# 3D Multi-organ Segmentation with UNETR  (BTCV Challenge)\n",
    "# PyTorch Lightning Tutorial\n",
    "\n",
    "\n",
    "本教程演示了如何将 MONAI 与 PyTorch Lightning 框架结合使用，以使用 BTCV 挑战数据集构建 UNETR 关于多器官分割任务的训练工作流程。\n",
    "\n",
    "![image](../figures/btcv_transformer.png)\n",
    "\n",
    "它包含以下功能：\n",
    "1. 字典格式数据的转换。\n",
    "2. 根据 MONAI 转换 API 定义新的转换。\n",
    "3. 加载带有元数据的 Nifti 图像，加载图像列表并将它们堆叠起来。\n",
    "4. 随机调整强度以进行数据增强。\n",
    "5. 缓存 IO 和转换以加速训练和验证。\n",
    "6. 3D UNETR模型，骰子损失函数，多器官分割任务的平均骰子指标。\n",
    "\n",
    "The dataset comes from https://www.synapse.org/#!Synapse:syn3193805/wiki/217752.  \n",
    "\n",
    "Under Institutional Review Board (IRB) supervision, 50 abdomen CT scans of were randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. The 50 scans were captured during portal venous contrast phase with variable volume sizes (512 x 512 x 85 - 512 x 512 x 198) and field of views (approx. 280 x 280 x 280 mm3 - 500 x 500 x 650 mm3). The in-plane resolution varies from 0.54 x 0.54 mm2 to 0.98 x 0.98 mm2, while the slice thickness ranges from 2.5 mm to 5.0 mm. \n",
    "\n",
    "Target: 13 abdominal organs including 1. Spleen 2. Right Kidney 3. Left Kidney 4.Gallbladder 5.Esophagus 6. Liver 7. Stomach 8.Aorta 9. IVC 10. Portal and Splenic Veins 11. Pancreas 12 Right adrenal gland 13 Left adrenal gland.\n",
    "\n",
    "Modality: CT\n",
    "Size: 30 3D volumes (24 Training + 6 Testing)  \n",
    "Challenge: BTCV MICCAI Challenge\n",
    "\n",
    "The following figure shows image patches with the organ sub-regions that are annotated in the CT (top left) and the final labels for the whole dataset (right).\n",
    "\n",
    "Data, figures and resources are taken from: \n",
    "\n",
    "\n",
    "1. [UNETR: Transformers for 3D Medical Image Segmentation](https://arxiv.org/abs/2103.10504)\n",
    "\n",
    "2. [High-resolution 3D abdominal segmentation with random patch network fusion (MIA)](https://www.sciencedirect.com/science/article/abs/pii/S1361841520302589)\n",
    "\n",
    "3. [Efficient multi-atlas abdominal segmentation on clinically acquired CT with SIMPLE context learning (MIA)](https://www.sciencedirect.com/science/article/abs/pii/S1361841515000766?via%3Dihub)\n",
    "\n",
    "\n",
    "![image](../figures/BTCV_organs.png)\n",
    "\n",
    "\n",
    "\n",
    "图像贴片显示受试者的解剖结构，包括：\n",
    "1.大器官：脾、肝、胃。\n",
    "2.较小的器官：胆囊、食道、肾脏、胰腺。\n",
    "3.血管组织：主动脉、IVC、P&S静脉。\n",
    "4.腺体：左右肾上腺\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/unetr_btcv_segmentation_3d_lightning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -onai (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -onai (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "DEPRECATION: matlabruntimeforpython R2017a has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of matlabruntimeforpython or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, einops]\"\n",
    "!pip install -q pytorch-lightning~=2.0\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2418\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.0.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 629f558991fcbb7d84d8d9dfd768ee099b3ed24c\n",
      "MONAI __file__: c:\\ProgramData\\Miniconda3\\lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.12.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: 2.10.1\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.0\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    list_data_collate,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "您可以使用“MONAI_DATA_DIRECTORY”环境变量指定目录。 \n",
    "这允许您保存结果并重复使用下载内容。 \n",
    "如果未指定，将使用临时目录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and format in the folder.\n",
    "    1. Download dataset from here: https://www.synapse.org/#!Synapse:syn3193805/wiki/89480\\n\n",
    "    2. Put images in the ./data/imagesTr\n",
    "    3. Put labels in the ./data/labelsTr\n",
    "    4. make JSON file accordingly: ./data/dataset_0.json\n",
    "    Example of JSON file:\n",
    "     {\n",
    "    \"description\": \"btcv yucheng\",\n",
    "    \"labels\": {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"spleen\",\n",
    "        \"2\": \"rkid\",\n",
    "        \"3\": \"lkid\",\n",
    "        \"4\": \"gall\",\n",
    "        \"5\": \"eso\",\n",
    "        \"6\": \"liver\",\n",
    "        \"7\": \"sto\",\n",
    "        \"8\": \"aorta\",\n",
    "        \"9\": \"IVC\",\n",
    "        \"10\": \"veins\",\n",
    "        \"11\": \"pancreas\",\n",
    "        \"12\": \"rad\",\n",
    "        \"13\": \"lad\"\n",
    "    },\n",
    "    \"licence\": \"yt\",\n",
    "    \"modality\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"name\": \"btcv\",\n",
    "    \"numTest\": 20,\n",
    "    \"numTraining\": 80,\n",
    "    \"reference\": \"Vanderbilt University\",\n",
    "    \"release\": \"1.0 06/08/2015\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"test\": [\n",
    "        \"imagesTs/img0061.nii.gz\",\n",
    "        \"imagesTs/img0062.nii.gz\",\n",
    "        \"imagesTs/img0063.nii.gz\",\n",
    "        \"imagesTs/img0064.nii.gz\",\n",
    "        \"imagesTs/img0065.nii.gz\",\n",
    "        \"imagesTs/img0066.nii.gz\",\n",
    "        \"imagesTs/img0067.nii.gz\",\n",
    "        \"imagesTs/img0068.nii.gz\",\n",
    "        \"imagesTs/img0069.nii.gz\",\n",
    "        \"imagesTs/img0070.nii.gz\",\n",
    "        \"imagesTs/img0071.nii.gz\",\n",
    "        \"imagesTs/img0072.nii.gz\",\n",
    "        \"imagesTs/img0073.nii.gz\",\n",
    "        \"imagesTs/img0074.nii.gz\",\n",
    "        \"imagesTs/img0075.nii.gz\",\n",
    "        \"imagesTs/img0076.nii.gz\",\n",
    "        \"imagesTs/img0077.nii.gz\",\n",
    "        \"imagesTs/img0078.nii.gz\",\n",
    "        \"imagesTs/img0079.nii.gz\",\n",
    "        \"imagesTs/img0080.nii.gz\"\n",
    "    ],\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0001.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0001.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0002.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0002.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0003.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0003.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0004.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0004.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0005.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0005.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0006.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0006.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0007.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0007.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0008.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0008.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0009.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0009.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0010.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0010.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0021.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0021.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0022.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0022.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0023.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0023.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0024.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0024.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0025.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0025.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0026.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0026.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0027.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0027.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0028.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0028.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0029.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0029.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0030.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0030.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0031.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0031.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0032.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0032.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0033.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0033.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0034.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0034.nii.gz\"\n",
    "        }\n",
    "    ],\n",
    "    \"validation\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0035.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0035.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0036.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0036.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0037.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0037.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0038.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0038.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0039.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0039.nii.gz\"\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0040.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0040.nii.gz\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LightningModule (transform, network)\n",
    "LightningModule 包含训练代码的重构。以下模块是对 spleen_segmentation_3d.ipynb 中代码的重构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self._model = UNETR(\n",
    "            in_channels=1,\n",
    "            out_channels=14,\n",
    "            img_size=(96, 96, 96),\n",
    "            feature_size=16,\n",
    "            hidden_size=768,\n",
    "            mlp_dim=3072,\n",
    "            num_heads=12,\n",
    "            pos_embed=\"perceptron\",\n",
    "            norm_name=\"instance\",\n",
    "            res_block=True,\n",
    "            conv_block=True,\n",
    "            dropout_rate=0.0,\n",
    "        ).to(device)\n",
    "\n",
    "        self.loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=14)\n",
    "        self.post_label = AsDiscrete(to_onehot=14)\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.max_epochs = 2\n",
    "        self.check_val = 3\n",
    "        self.warmup_epochs = 2\n",
    "        self.metric_values = []\n",
    "        self.epoch_loss_values = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # prepare data\n",
    "        data_dir = \"D:/online-study_Flask/data/\"\n",
    "        split_json = \"dataset_0.json\"\n",
    "        datasets = data_dir + split_json\n",
    "        datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "        val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-175,\n",
    "                    a_max=250,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                RandCropByPosNegLabeld(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    label_key=\"label\",\n",
    "                    spatial_size=(96, 96, 96),\n",
    "                    pos=1,\n",
    "                    neg=1,\n",
    "                    num_samples=4,\n",
    "                    image_key=\"image\",\n",
    "                    image_threshold=0,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[2],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                RandShiftIntensityd(\n",
    "                    keys=[\"image\"],\n",
    "                    offsets=0.10,\n",
    "                    prob=0.50,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-175,\n",
    "                    a_max=250,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_ds = CacheDataset(\n",
    "            data=datalist,\n",
    "            transform=train_transforms,\n",
    "            cache_num=24,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=8,\n",
    "        )\n",
    "        self.val_ds = CacheDataset(\n",
    "            data=val_files,\n",
    "            transform=val_transforms,\n",
    "            cache_num=6,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.val_ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self._model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = (batch[\"image\"], batch[\"label\"])\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def on_train_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.epoch_loss_values.append(avg_loss.detach().cpu().numpy())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        roi_size = (96, 96, 96)\n",
    "        sw_batch_size = 2\n",
    "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self.forward)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "        self.validation_step_outputs.append(d)\n",
    "        return d\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in self.validation_step_outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        tensorboard_logs = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_loss\": mean_val_loss,\n",
    "        }\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.metric_values.append(mean_val_dice)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "        return {\"log\": tensorboard_logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading dataset: 100%|██████████| 1/1 [00:04<00:00,  4.29s/it]\n",
      "Loading dataset: 100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | _model        | UNETR      | 92.8 M\n",
      "1 | loss_function | DiceCELoss | 0     \n",
      "---------------------------------------------\n",
      "92.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.8 M    Total params\n",
      "371.136   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [01:20<00:00,  0.01it/s]current epoch: 0 current mean dice: 0.0108\n",
      "best mean dice: 0.0108 at epoch: 0\n",
      "Epoch 0:   0%|          | 0/1 [09:13<?, ?it/s]                             \n",
      "Epoch 0:   0%|          | 0/1 [05:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 226492416 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\online-study_Flask\\3d.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m pytorch_lightning\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39mnet\u001b[39m.\u001b[39mmax_epochs,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     check_val_every_n_epoch\u001b[39m=\u001b[39mnet\u001b[39m.\u001b[39mcheck_val,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(net)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    546\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1034\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[0;32m    251\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[0;32m    192\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    269\u001b[0m     trainer,\n\u001b[0;32m    270\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    271\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    272\u001b[0m     batch_idx,\n\u001b[0;32m    273\u001b[0m     optimizer,\n\u001b[0;32m    274\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1303\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1270\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1271\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \n\u001b[0;32m   1302\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(optimizer, model\u001b[39m=\u001b[39mmodel, closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\optim\\adamw.py:148\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 148\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    151\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[0;32m    317\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32md:\\online-study_Flask\\3d.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     images, labels \u001b[39m=\u001b[39m (batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(images)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(output, labels)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     tensorboard_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m: loss\u001b[39m.\u001b[39mitem()}\n",
      "\u001b[1;32md:\\online-study_Flask\\3d.ipynb 单元格 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/online-study_Flask/3d.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\monai\\networks\\nets\\unetr.py:219\u001b[0m, in \u001b[0;36mUNETR.forward\u001b[1;34m(self, x_in)\u001b[0m\n\u001b[0;32m    217\u001b[0m dec2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder4(dec3, enc3)\n\u001b[0;32m    218\u001b[0m dec1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder3(dec2, enc2)\n\u001b[1;32m--> 219\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder2(dec1, enc1)\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(out)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\monai\\networks\\blocks\\unetr_block.py:85\u001b[0m, in \u001b[0;36mUnetrUpBlock.forward\u001b[1;34m(self, inp, skip)\u001b[0m\n\u001b[0;32m     83\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransp_conv(inp)\n\u001b[0;32m     84\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((out, skip), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block(out)\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\monai\\networks\\blocks\\dynunet_block.py:106\u001b[0m, in \u001b[0;36mUnetResBlock.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    104\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(out)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mconv3\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 106\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(residual)\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnorm3\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    108\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(residual)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[0;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[0;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[0;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\monai\\data\\meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 282\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, kwargs)\n\u001b[0;32m    283\u001b[0m \u001b[39m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m#     return ret\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1297\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 226492416 bytes."
     ]
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "net = Net()\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    devices=1,\n",
    "    accelerator='cpu',\n",
    "    max_epochs=net.max_epochs,\n",
    "    check_val_every_n_epoch=net.check_val,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_num = 250\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(net.epoch_loss_values))]\n",
    "y = net.epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(net.metric_values))]\n",
    "y = net.metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用输入图像和标签检查最佳模型输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_map = {\n",
    "    \"img0035.nii.gz\": 170,\n",
    "    \"img0036.nii.gz\": 230,\n",
    "    \"img0037.nii.gz\": 204,\n",
    "    \"img0038.nii.gz\": 204,\n",
    "    \"img0039.nii.gz\": 204,\n",
    "    \"img0040.nii.gz\": 180,\n",
    "}\n",
    "case_num = 4\n",
    "net.eval()\n",
    "net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_name = os.path.split(net.val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "    img = net.val_ds[case_num][\"image\"]\n",
    "    label = net.val_ds[case_num][\"label\"]\n",
    "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
    "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
    "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, net, overlap=0.8)\n",
    "    plt.figure(\"check\", (18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_map[img_name]], cmap=\"gray\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_map[img_name]])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"output\")\n",
    "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, slice_map[img_name]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
